{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import onnx\n",
    "\n",
    "#model = onnx.load(\"C:/Users/tyra_/.insightface/models/adaface_custom/adaface_model.onnx\")\n",
    "#print(\"Model Outputs:\")\n",
    "#for output in model.graph.output:\n",
    "#    print(output.name, output.type)\n",
    "    \n",
    "# Remove the second output (shape (1, 1))\n",
    "#model.graph.output.remove(model.graph.output[1])\n",
    "\n",
    "# Save the modified model\n",
    "#onnx.save(model, \"C:/Users/tyra_/.insightface/models/adaface_custom/adaface_model_modified.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\tyra_/.insightface\\models\\adaface_custom\\adaface_model_modified.onnx recognition ['batch_size', 3, 112, 112] 127.5 127.5\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\tyra_/.insightface\\models\\adaface_custom\\det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "set det-size: (640, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import insightface\n",
    "from insightface.app import FaceAnalysis\n",
    "from insightface.data import get_image as ins_get_image\n",
    "\n",
    "#First create handler with our own Adaface model\n",
    "handler = insightface.model_zoo.get_model('C:/Users/tyra_/.insightface/models/adaface_custom/adaface_model_modified.onnx')\n",
    "handler.prepare(ctx_id=0)\n",
    "\n",
    "\n",
    "#Then use FaceAnalysis to extract face\n",
    "app = FaceAnalysis(name='adaface_custom') # changed so that it used Adaface Instead of Arcface as feature extractor. \n",
    "app.prepare(ctx_id=0, det_size=(640, 640))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"Images\\img1.jpeg\")\n",
    "image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)  # Convert to RGB\n",
    "image = cv2.resize(image, (112, 112)) \n",
    "features_one = app.get(image)\n",
    "\n",
    "image_two = cv2.imread(\"Images\\img2.jpeg\")\n",
    "image_two = cv2.cvtColor(image_two, cv2.COLOR_RGB2BGR)  # Convert to RGB\n",
    "image_two = cv2.resize(image_two, (112, 112)) \n",
    "features_two = app.get(image_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity score:  0.57126004\n"
     ]
    }
   ],
   "source": [
    "# Extract embeddings from the features list\n",
    "embeddings_one = np.array([face['embedding'] for face in features_one])\n",
    "embeddings_two = np.array([face['embedding'] for face in features_two])\n",
    "\n",
    "# Print the result\n",
    "#print(embeddings_one)\n",
    "\n",
    "\n",
    "similarity = handler.compute_sim(embeddings_one, embeddings_two)\n",
    "print(\"similarity score: \", similarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "#Images to compare\n",
    "#image = Image.open('Images\\Lord_farquaad.jpg')\n",
    "\n",
    "image_path = \"Images\\img1.jpeg\"\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.resize(image, (112, 112))  # ArcFace expects 112x112 input\n",
    "#image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "\n",
    "image_path_two = \"Images\\img2.jpeg\"\n",
    "image_two = cv2.imread(image_path_two)\n",
    "image_two = cv2.resize(image_two, (112, 112))  # ArcFace expects 112x112 input\n",
    "\n",
    "image_path_three = \"Images\\img3.jpeg\"\n",
    "image_three = cv2.imread(image_path_three)\n",
    "image_three = cv2.resize(image_three, (112, 112))  # ArcFace expects 112x112 input\n",
    "#image_two = cv2.cvtColor(image_two, cv2.COLOR_BGR2RGB)  # Convert to RGB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bbox': array([39.562286, 28.807713, 67.334435, 79.791435], dtype=float32), 'kps': array([[46.40921 , 49.164707],\n",
      "       [59.562286, 49.261745],\n",
      "       [52.843002, 61.389767],\n",
      "       [48.974556, 69.46752 ],\n",
      "       [57.32276 , 69.64942 ]], dtype=float32), 'det_score': 0.8027915}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from insightface.app import common\n",
    "\n",
    "image = cv2.imread(\"Images\\img1.jpeg\")\n",
    "image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)  # Convert to RGB\n",
    "image = cv2.resize(image, (112, 112)) \n",
    "face_inputs = app.get(image)\n",
    "\n",
    "# Extract the first dictionary from the list\n",
    "data = face_inputs[0]\n",
    "\n",
    "# Assign values to variables\n",
    "bbox = data['bbox']\n",
    "kps = data['kps']\n",
    "det_score = data['det_score']\n",
    "\n",
    "face = common.Face(bbox=bbox, kps=kps, det_score=det_score)\n",
    "print(face) #The output with \"detection only\" becomes bbox, kps, and det_score wich is needed for Face()\n",
    "embedding_one = torch.tensor(handler.get(image, face)) #confusing but this get is not the same as the other get, this one needs two inputs get(img, face)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bbox': array([26.311981, 12.46217 , 75.01176 , 71.89394 ], dtype=float32), 'kps': array([[39.11384 , 37.930767],\n",
      "       [60.62524 , 38.09466 ],\n",
      "       [49.422993, 49.812588],\n",
      "       [40.67221 , 57.591698],\n",
      "       [59.28077 , 57.69183 ]], dtype=float32), 'det_score': 0.81695795}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from insightface.app import common\n",
    "\n",
    "image = cv2.imread('Images\\img2.jpeg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)  # Convert to BGR\n",
    "image = cv2.resize(image, (112, 112)) #cropped to 112x112x3 size whose color channel is BGR order.\n",
    "face_inputs = app.get(image)\n",
    "\n",
    "# Extract the first dictionary from the list\n",
    "data = face_inputs[0]\n",
    "\n",
    "# Assign values to variables\n",
    "bbox = data['bbox']\n",
    "kps = data['kps']\n",
    "det_score = data['det_score']\n",
    "\n",
    "face = common.Face(bbox=bbox, kps=kps, det_score=det_score)\n",
    "print(face) #The output with \"detection only\" becomes bbox, kps, and det_score wich is needed for Face()\n",
    "embedding_two = torch.tensor(handler.get(image, face)) #confusing but this get is not the same as the other get, this one needs two inputs get(img, face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bbox': array([26.281944,  9.186456, 54.5598  , 56.7966  ], dtype=float32), 'kps': array([[31.651106, 28.607227],\n",
      "       [44.028976, 27.502928],\n",
      "       [36.842247, 38.38748 ],\n",
      "       [34.849167, 44.827473],\n",
      "       [44.728664, 43.932957]], dtype=float32), 'det_score': 0.86501145}\n"
     ]
    }
   ],
   "source": [
    "image = cv2.imread('Images\\img3.jpeg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)  # Convert to BGR\n",
    "image = cv2.resize(image, (112, 112)) #cropped to 112x112x3 size whose color channel is BGR order.\n",
    "face_inputs = app.get(image)\n",
    "\n",
    "# Extract the first dictionary from the list\n",
    "data = face_inputs[0]\n",
    "\n",
    "# Assign values to variables\n",
    "bbox = data['bbox']\n",
    "kps = data['kps']\n",
    "det_score = data['det_score']\n",
    "\n",
    "face = common.Face(bbox=bbox, kps=kps, det_score=det_score)\n",
    "print(face) #The output with \"detection only\" becomes bbox, kps, and det_score wich is needed for Face()\n",
    "embedding_three = torch.tensor(handler.get(image, face)) #confusing but this get is not the same as the other get, this one needs two inputs get(img, face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.)\n"
     ]
    }
   ],
   "source": [
    "#similarity = handler.compute_sim(embedding_one, embedding_two)\n",
    "#print(\"similarity score: \", similarity)\n",
    "features = [embedding_one,embedding_two, embedding_three]\n",
    "similarity_scores = torch.cat(features) @ torch.cat(features).T\n",
    "print(similarity_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "old-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
